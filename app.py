import streamlit as st
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import requests
from requests.exceptions import RequestException
import time  # Ajoutez ceci avec les autres imports
from tenacity import retry, stop_after_attempt, wait_exponential  # Pour les r√©essais automatiques

st.set_page_config(page_title="MedRegBot", layout="wide")

# ---------------------- Logos ----------------------
col1, col2, col3 = st.columns([1, 4, 1])
with col1:
    st.image("logo_left.jpeg", width=120)
with col2:
    st.markdown("<h1 style='text-align: center; color: #003366;'>MedRegBot</h1>", unsafe_allow_html=True)
    st.markdown("<h4 style='text-align: center; color: #005580;'>Votre assistant IA pour la r√©glementation des dispositifs m√©dicaux</h4>", unsafe_allow_html=True)
with col3:
    st.image("logo_right.jpeg", width=100)
st.markdown("---")

# ---------------------- Chargement de la base de donn√©es ----------------------
@st.cache_resource
def load_data():
    with open("embeddings_from_drive.pkl", "rb") as f:
        data = pickle.load(f)
    texts = [d["text"] for d in data]
    sources = [d.get("source", "Inconnu") for d in data]
    embeddings = np.array([d["embedding"] for d in data])
    return texts, sources, embeddings

texts, sources, embeddings = load_data()
model = SentenceTransformer("sentence-transformers/LaBSE")

# ---------------------- Style ----------------------
whatsapp_style = '''
<style>
.user-msg {
    background-color: #ffffff;
    color: black;
    padding: 10px 15px;
    border-radius: 15px;
    max-width: 70%;
    margin: 10px 0;
    margin-left: auto;
    text-align: right;
}
.bot-msg {
    background-color: #e6f0ff;
    color: black;
    padding: 10px 15px;
    border-radius: 15px;
    max-width: 70%;
    margin: 10px 0;
    margin-right: auto;
    text-align: left;
}
.loading-msg {
    color: #666;
    font-style: italic;
}
</style>
'''
st.markdown(whatsapp_style, unsafe_allow_html=True)

# ---------------------- Session state ----------------------
if "history" not in st.session_state:
    st.session_state.history = []

# ---------------------- Interface de chat ----------------------
st.markdown("### üí¨ Chat avec MedRegBot")

for role, msg in st.session_state.history:
    if role == "user":
        st.markdown(f"<div class='user-msg'>{msg}</div>", unsafe_allow_html=True)
    elif role == "bot":
        st.markdown(f"<div class='bot-msg'>{msg}</div>", unsafe_allow_html=True)
    elif role == "loading":
        st.markdown(f"<div class='loading-msg'>{msg}</div>", unsafe_allow_html=True)

user_input = st.chat_input("Posez votre question ici...")
if user_input:
    # √âtape 1: Ajout du message utilisateur
    st.session_state.history.append(("user", user_input))
    loading_msg = st.empty()  # Cr√©e un placeholder pour le message de chargement
    
    try:
        # √âtape 2: Recherche contextuelle (rapide)
        loading_msg.markdown("<div class='loading-msg'>üîç Recherche dans les textes r√©glementaires...</div>", unsafe_allow_html=True)
        question_embedding = model.encode([user_input])
        similarities = cosine_similarity(question_embedding, embeddings)[0]
        top_indices = similarities.argsort()[-3:][::-1]
        context = "\n".join([texts[i] for i in top_indices])
        
        # √âtape 3: Appel API avec gestion du temps
        loading_msg.markdown("<div class='loading-msg'>üß† Analyse par l'expert IA...</div>", unsafe_allow_html=True)
        start_time = time.time()
        
        response = query_mistral(user_input, context)
        
        # Feedback si d√©lai d√©pass√©
        if time.time() - start_time > 15:
            loading_msg.markdown("<div class='loading-msg'>‚è≥ L'analyse prend plus de temps que pr√©vu...</div>", unsafe_allow_html=True)
        
        # √âtape 4: Traitement r√©ponse
        if response.status_code == 200:
            generated_text = response.json()[0]["generated_text"]
            answer = generated_text.split("[/INST]")[-1].strip()
        elif response.status_code == 503:
            answer = "‚ö†Ô∏è Le syst√®me est occup√©. Merci de reformuler votre question."
        else:
            answer = f"‚ö†Ô∏è R√©ponse partielle (code {response.status_code})"
            
    except Exception as e:
        answer = f"‚ö†Ô∏è Service temporairement indisponible: {str(e)}"
    finally:
        loading_msg.empty()  # Supprime le message de chargement
    
    # √âtape 5: Affichage r√©ponse
    st.session_state.history.append(("bot", answer))

        # --------- Appel √† l'API Hugging Face (Mistral gratuit) ---------
        # ---------------------- Configuration API ----------------------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def query_mistral(prompt, context):
    API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1"
    headers = {
        "Authorization": f"Bearer {st.secrets.get('HF_TOKEN', '')}",
        "Content-Type": "application/json"
    }
    
    full_prompt = f"""<s>[INST] Tu es un expert en r√©glementation m√©dicale. 
    R√©ponds en fran√ßais en 3-5 phrases maximum en t'appuyant sur ce contexte:
    
    Contexte: {context}
    
    Question: {prompt} [/INST]"""
    
    response = requests.post(
        API_URL,
        headers=headers,
        json={
            "inputs": full_prompt,
            "parameters": {
                "max_new_tokens": 350,  # R√©ponse plus courte
                "temperature": 0.7  # Moins de cr√©ativit√©, plus factuel
            }
        },
        timeout=20  # Timeout plus court
    )
    return response

        # Mise √† jour du message de chargement
        st.session_state.history = [h for h in st.session_state.history if h[0] != "loading"]
        st.session_state.history.append(("loading", "G√©n√©ration de la r√©ponse..."))
        st.rerun()

        with st.spinner(""):
            response = requests.post(
                API_URL,
                headers=headers,
                json={
                    "inputs": prompt,
                    "parameters": {"max_new_tokens": 500}
                },
                timeout=45
            )
        
        if response.status_code == 200:
            generated_text = response.json()[0]["generated_text"]
            answer = generated_text.split("[/INST]")[-1].strip()
        elif response.status_code == 503:
            answer = "‚ö†Ô∏è Le mod√®le est en cours de chargement. Merci de r√©essayer dans 30 secondes."
        else:
            answer = f"‚ö†Ô∏è Erreur API (code {response.status_code}): {response.text[:200]}..."

    except RequestException as e:
        answer = f"‚ö†Ô∏è Erreur de connexion: {str(e)}"
    except Exception as e:
        answer = f"‚ö†Ô∏è Erreur inattendue: {str(e)}"

    # Mise √† jour finale de l'historique
    st.session_state.history = [h for h in st.session_state.history if h[0] != "loading"]
    st.session_state.history.append(("bot", answer))
